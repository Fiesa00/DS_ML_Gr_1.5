{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar Notizen\n",
    "\n",
    "#### Woche 5: Einführung Maschinelles Lernen\n",
    "##### Lineares Modell\n",
    "\n",
    "- Lineares Modell als Baseline Modell\n",
    "    - einziger Hyperparameter -> Lernrate\n",
    "- Zielvariable --> Umsatz (Vorhersage)\n",
    "- Daten werden aufgeteilt (Trainingsdaten, Validierungsdaten, Testdaten)\n",
    "- Keine Randomisierung möglich (Bäckereidaten zeitabhängig!)<br>\n",
    "- Optimierungsfunktion\n",
    "    - mehrere lokale Maxima, keine globle Minima --> Methode der kleinsten Quadrate nicht geeignet\n",
    "    - Verfahren des absteigenden Gradienten (iteratives Verfahren)\n",
    "    - Gradient --> Richtung der größten Änderung einer Funktion\n",
    "    - **Forward Propagation**: Berechnung der Kostenfunktion\n",
    "    - **Backward Propagation**: Optimierung der Kostenfunktion\n",
    "- Baseline_Modell erstellen!\n",
    "- Warengruppe als Feature!!\n",
    "- Vorhersage --> Was für Warengruppe soll am nächsten Tag produziert werden --> Vorhersagen pro Warengruppe\n",
    "\n",
    "### Woche 6: Overfitting\n",
    "##### ML Begriffe\n",
    "- Umsatz als Targetvariable -> Modellziel: Nur Umsatzvorhersage\n",
    "- Umsatzvorhersage nach Warengruppe und Wochentage\n",
    "- Datum als Hilfskonstruktion --> Reihenfolge der Tage gehen nicht ein ins Modell\n",
    "- **Optimisierungsfunktion** --> Gradient Descent (Hyperparameter: Lernrate -> Wie schnell erreiche ich das Minimum?)\n",
    "- automatisierte Optimisierung der Modellparameters (backward propagation) -> nach definierter Lernrate\n",
    "    - Werden mit Backward Prop angepasst: Modellparameter & Hyperparameter (Knotenpunkte im neuronalen Netzwerk)\n",
    "    - Parameter werden gewichtet (Faktoren) -> Kosten\n",
    "- Regularisierung: Modell lässt selber Variablen raus, die unrelevant sind -> Regularisierung bestraft Verwendung von Variablen über Regularisierungsparameter (Bestrafungsgröße, (Hyperparameter)) (Kleiner Gewicht -> kleine Kostenfunktion, aber Informationsverlust der Variable)\n",
    "- Kostenfunktion abhängig von den Gewichten\n",
    "##### Batches\n",
    "- Datensatz bzw. Beobachtungen werden in batches angelegt -> 32 Datenpunkte pro Opitimierungskreis\n",
    "- Batchsize als Hyperparameter\n",
    "- Epoche: Gleiche Daten -> nächster Optimisierungsschritt wieder mit gleichen Daten\n",
    "- Bachsize: Anzahl an neuen Daten, die das Modell optimieren muss -> geringeres Overfitting-Potenzial\n",
    "##### Modellgütekriterien\n",
    "- errors (auch residuals) = predicted - actual \n",
    "- Loss-Funktion \n",
    "    - mae (auch accuracy, nicht relativ)= mean(abs(error)) -> \n",
    "    - mape (prozentualler Fehler, sollte < 20-30% sein) = mean(abs(errors/actual))\n",
    "    - mse = mean(errors^2) -> bestraft outlier stärker (größere Werte verursachen schneller Kosten)\n",
    "    - rmse = sqrt(mean(errors^2))\n",
    "    - rse (realtive squared error) = sum(errors^2)/sum((actual-mean(actual))^2)\n",
    "    - r^2 = 1 - rse\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Woche 7: Neuronales Netzwerk\n",
    "\n",
    "##### Momentum\n",
    "- 'Trägheitsparameter' für die Optimierung\n",
    "- Richtung wird nur minimal angepasst, \n",
    "- batch-size = 1 --> Trägheit höher setzen, um over-fitting zu vermeiden\n",
    "##### Optimizier\n",
    "- Lernrate\n",
    "- Anzahl Hidden Layer, Anzahl Neuronen je Hidden Layer, Typ der Hidden Layer\n",
    "- batchsize\n",
    "##### Programmieren:\n",
    "- dense() --> Anzahl der Neuronen in einem Layer\n",
    "- Interaktionen müssen im linearen Modell händisch definiert werden\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Woche 8: Fehlende Werte\n",
    "\n",
    "- Fehlende Daten aus Datensatz löschen\n",
    "- One-Hot bzw. Dummie Codierung --> kategorielle Variablen in numerische Variablen\n",
    "- Momentum: Trägheit: langsamere Änderung der Richtung der Gewichte (Konvergenzrichtung)\n",
    "- Batch-Normalisierung: Technisch, Damit Modell stabiler und schneller läuft\n",
    "- Dropout: Deaktivierung von Neuronen im vorgegangenen Layer\n",
    "- Aktivierungsfunktion: nur bei neuronalen Netzwerken. Von linearen Modell zum nicht-linearen Modell bzw. relu\n",
    "- Regularisierung nur bei linearer Regression\n",
    "\n",
    "##### Neuronales Netztwerk\n",
    "\n",
    "**Dropout**\n",
    "- layer: Deaktiviert Neuronen in einem Layer mit bestimmte Prozent --> Jedes Mal werden andere Neuronen und andere Neuronenazhal rausgeschmießen (individuell) --> Informationen werden auf die Neuronen verteilit (robuster) --> overfitting schwieriger --> i.d.R. 0,2 bis 0,3\n",
    "- Zufallsziehung\n",
    "- Gibt keinen Paramenter, dropout layer als Form der Regularisierung\n",
    "- Wird nur auf dem Trainingsdatensatz angewendet!\n",
    "\n",
    "##### Fehlende Werte\n",
    "\n",
    "- Warum fehlt ein Wert?\n",
    "- Daten aus verschiedenen Quellen --> verschiedene Datenreihen --> theoretisch fehlende Werte\n",
    "- technische Probleme\n",
    "- Kategorien:\n",
    "- Missing Completely at random -> Sensor geht kaputt, \n",
    "- missing at random -> Wartungsarbeiten (bsw. nach bestimmten Muster, nie am Wochenende etc.)\n",
    "- missing not at random -> Systematische Sensorfehler (bspw. - 20°C schaltet sich Sensor aus)\n",
    "##### Lösungswege\n",
    "- listwise deletion --> Fälle werden gelöscht\n",
    "- \"donor-badsed\" --> Mittelwertschätzung (Median or Mode)\n",
    "**Hot-Deck Imputation (nach Ähnlichkeiten)**\n",
    "- nach Domäne (Wert von der letzten Variable mit gleichen Eigenschaften wie die Variable mit dem fehlenden Wert)\n",
    "**k Nearest Neighbors (minimaler Abstand)**\n",
    "- Distanzen zu den anderen Fällen --> Wert vom nächsten, der ähnlich ist oder zufällige Lösung von einer Anzahl an Nachbarn\n",
    "**iterative Regression**\n",
    " - Modellbasierte Imputation\n",
    " - Vorhersage für Variablen, die fehlen -> als nächstes werden vorhergesagten Daten genutzt für die nächste Vorhersage --> Erneute Vorhersage der ersten Werte usw. --> bis keine Änderung mehr eintritt\n",
    " - Korrelation wird unnatülich erhöht --> nicht für at random geeignet\n",
    "**Multiple Imputation**\n",
    "\n",
    "Siehe Notebook: missing_value_imputation.ipynb\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Woche 9: Zeitreihenanalysen\n",
    "\n",
    "**Follow-up**\n",
    "\n",
    "- Bei vielen missing values kann es sinnvoll, Modell Information mitzugeben, welche Daten imputiert sind (als Dummi Feature).\n",
    "- Drop-out layer --> trainings lost höher, weil weniger Knoten zur Berechnung vorhanden im Gegensatz zum Validierungs lost\n",
    "- learning scalutor für den Datensatz --> kann in einer Zeile hinzugefügt werden --> nach jedem Step wird Lernrate etwas runter gesetzt \n",
    "\n",
    "**Zeitreihen**\n",
    "- Nach non-stationarity (bspw. big event) ändert sich die Form der Zeitreihe fast komplett\n",
    "- Autokorrelation: Der Wert von gestern korreliert mit dem Wert von morgen --> Differencing \n",
    "- Differencing: Aktuelle Beobachtung minus Wert von vorherigen Beobachten (Differenzen von Mustern) --> Saisonality wird entfernt\n",
    "    - Zeitreihe wird zerlegt in unterschiedliche Effekte\n",
    "- Lag: Unterschied, den man abziehen möchte (z.B. Tage, Woche, Monate, Jahre etc.)\n",
    "- naive Forcasting: Werte aus dem vorherigen Beobachtung bestimmt die Prediction\n",
    "- Jährliche Saisonalität kann mit in Optimierung genommen werden\n",
    "- Die letzte  sieben Tage für die Prediction mit reinnehmen: Variable definieren mit Umsätze von Vortagen (Lag-Funktion)\n",
    "- Features sind unabhängig voneinander im aktuellen neuronalen Netzwerk (Reihenfolge der Variablen spielt keine Rolle)\n",
    "    - Gegenteil: Recurrent Neural net (RNN) --> Variablen /Features sind voneinander abhängig und geben ihre Informationen an nachfolgende Variable weiter --> Problem: für mehrere Parameter zeitgleich nicht geeignet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
