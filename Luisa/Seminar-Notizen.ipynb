{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar Notizen\n",
    "\n",
    "#### Einführung Maschinelles Lernen\n",
    "##### Lineares Modell\n",
    "\n",
    "- Lineares Modell als Baseline Modell\n",
    "    - einziger Hyperparameter -> Lernrate\n",
    "- Zielvariable --> Umsatz (Vorhersage)\n",
    "- Daten werden aufgeteilt (Trainingsdaten, Validierungsdaten, Testdaten)\n",
    "- Keine Randomisierung möglich (Bäckereidaten zeitabhängig!)<br>\n",
    "- Optimierungsfunktion\n",
    "    - mehrere lokale Maxima, keine globle Minima --> Methode der kleinsten Quadrate nicht geeignet\n",
    "    - Verfahren des absteigenden Gradienten (iteratives Verfahren)\n",
    "    - Gradient --> Richtung der größten Änderung einer Funktion\n",
    "    - **Forward Propagation**: Berechnung der Kostenfunktion\n",
    "    - **Backward Propagation**: Optimierung der Kostenfunktion\n",
    "- Baseline_Modell erstellen!\n",
    "- Warengruppe als Feature!!\n",
    "- Vorhersage --> Was für Warengruppe soll am nächsten Tag produziert werden --> Vorhersagen pro Warengruppe\n",
    "\n",
    "### Overfitting\n",
    "##### ML Begriffe\n",
    "- Umsatz als Targetvariable -> Modellziel: Nur Umsatzvorhersage\n",
    "- Umsatzvorhersage nach Warengruppe und Wochentage\n",
    "- Datum als Hilfskonstruktion --> Reihenfolge der Tage gehen nicht ein ins Modell\n",
    "- **Optimisierungsfunktion** --> Gradient Descent (Hyperparameter: Lernrate -> Wie schnell erreiche ich das Minimum?)\n",
    "- automatisierte Optimisierung der Modellparameters (backward propagation) -> nach definierter Lernrate\n",
    "    - Werden mit Backward Prop angepasst: Modellparameter & Hyperparameter (Knotenpunkte im neuronalen Netzwerk)\n",
    "    - Parameter werden gewichtet (Faktoren) -> Kosten\n",
    "- Regularisierung: Modell lässt selber Variablen raus, die unrelevant sind -> Regularisierung bestraft Verwendung von Variablen über Regularisierungsparameter (Bestrafungsgröße, (Hyperparameter)) (Kleiner Gewicht -> kleine Kostenfunktion, aber Informationsverlust der Variable)\n",
    "- Kostenfunktion abhängig von den Gewichten\n",
    "##### Batches\n",
    "- Datensatz bzw. Beobachtungen werden in batches angelegt -> 32 Datenpunkte pro Opitimierungskreis\n",
    "- Batchsize als Hyperparameter\n",
    "- Epoche: Gleiche Daten -> nächster Optimisierungsschritt wieder mit gleichen Daten\n",
    "- Bachsize: Anzahl an neuen Daten, die das Modell optimieren muss -> geringeres Overfitting-Potenzial\n",
    "##### Modellgütekriterien\n",
    "- errors (auch residuals) = predicted - actual \n",
    "- Loss-Funktion \n",
    "    - mae (auch accuracy, nicht relativ)= mean(abs(error)) -> \n",
    "    - mape (prozentualler Fehler, sollte < 20-30% sein) = mean(abs(errors/actual))\n",
    "    - mse = mean(errors^2) -> bestraft outlier stärker (größere Werte verursachen schneller Kosten)\n",
    "    - rmse = sqrt(mean(errors^2))\n",
    "    - rse (realtive squared error) = sum(errors^2)/sum((actual-mean(actual))^2)\n",
    "    - r^2 = 1 - rse\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
